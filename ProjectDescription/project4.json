{
  "title": "Partiallly Observable Markov Deciscion Process",
  "brief": "Research comparing Nash Q-learning, DQN, and fictitious play in multi-agent reinforcement learning environments.",
  "media": ["/img/POMDP/3.png", "/img/POMDP/1.png", "/img/POMDP/2.png"],
  "subtitles": [
    {
      "title": "Project Description",
      "content": "This is a Small Research for exploring Partiallly Observable Markov Games and multi-agent reinforcement learning, spanning several benchmark problems: a competitive Car-Truct 3*3 grid world, the classic Tiger POMDP, Rock-Paper-Scissors with learning dynamic, and a General DQN setup for single-0agent control. The goal of this research is to systematically compare game-theoretic learning (Nash Q-learning, Nash-QMDP, fictitious play) and deep RL methods (DQN with replay, target networks, ε-greedy exploration), under both full and partial observability."
    },
    {
      "title": "My Role",
      "content": "- Designed and implemented the Car–Truck grid world, Tiger POMDP, Rock–Paper–Scissors, and reward-only POMG transition models.\n- Implemented Nash Q-learning with linear-programming-based equilibrium computation and Nash-QMDP style value iteration.\n- Built DQN agents for the grid-world setting, including experience replay, target networks, and ε-greedy exploration.\n- Implemented fictitious play and best-response dynamics for Rock–Paper–Scissors to study empirical convergence to Nash equilibrium.\n- Constructed belief-state and reward-state transition matrices for partially observable settings and analyzed their effect on policy behavior.\n- Ran experiments to compare algorithms (Nash Q vs DQN vs fictitious play), inspected convergence and stability, and documented key findings."
    },
    {
      "title": "Tech Stack",
      "content": "Python, NumPy, SciPy, Matplotlib, Jupyter Notebooks, Linear Programming (PuLP), GitHub"
    }
  ]
}
